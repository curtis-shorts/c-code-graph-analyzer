\section{Related Work}
\label{sec:related}

Prior works have been done on the static analysis of HPC-specific codes. One example performed a compiler-based static analysis of HPC applications for identifying bugs in higher-level message passing architectures, such as MPI implementations, with the Clang/LLVM compiler \cite{sca_thesis}. Static analysis with GCC has also been done to the end of finding patterns across the field of HPC application's usage of MPI \cite{10.1007/978-3-030-17872-7_6}. This was done in order to better understand how hardware-software co-design can be leveraged for optimizing the production uses of HPC systems. The work presented in this paper defers from these examples in that it is looking at the middleware aspect of the HPC software stack which MPI is built on top of, whereas prior works look primarily at the the application codes that are built on top of MPI. No prior works on the static analysis of HPC middlewares were found in the literature.
%This key difference is what motivated the transition away from a compiler-based solution, as outlined through the discussion on middleware optimizations in Section \ref{sec:intro}.

Another work specific to HPC applications looked at using a combination of static and dynamic analysis techniques for analyzing how to refactor HPC applications to better understand it's codebase \cite{9460653}. Static analysis techniques can help to understand the architecture, but dynamic analysis is also necessary for understanding how performance bottlenecks can appear at runtime to identify performance optimizations. Determining critical paths for performance in the middleware can be done at the application level through identifying the frequency at which different APIs are called. This allows optimization efforts to be targeted at the areas of the code base which truly matter. The work presented in this paper excludes the dynamic analysis portion in favor of static analysis as the focus is on architecture recover rather than optimizing runtime performance. The prior works are also again focused on HPC application software rather than the middleware that sits below it.

Looking to other domains, general vulnerability detection tools for C are in circulation that utilize a combination of compiler and non-compiler based solutions \cite{vulnerability_detection_in_c}. The analysis of vulnerability detection puts a targeted focus on the implications of the code's structure as to how it will effect how it runs on the systems to detect runtime characteristics. The key differences between this prior work and what is presented in this paper are the locality of the observations and the direction of interest. The prior work is focused on localized patterns in the code that indicate vulnerabilities making the analysis locally scoped and runtime facing. In contrast, this work looks at the global patterns in order to extract patterns in the code-base making it globally scoped and source code facing.

Looking outside the specific applications of static analysis in the C language, one work looked at the architectural extraction of Python code-bases through the use of dependency graphs \ref{rao2020ac2understandingarchitectural}. The focus of this work was on architecture extraction for the purposes of understanding changes that occur between major releases in open source code-bases. The tool presented used a myriad of graphical user interface (GUI) tools in order to dynamically survey the differences between the new and old architectures including the hierarchical expansion and collapsing of directories and dependency graphs to view the calls between subroutines. This is different from the work presented in this paper as it is version-difference based rather than starting with no baseline for the architecture to be compared against. Parsing C also introduces the complexities with pre-processor directives that analyses of interpreted languages like Python don't need to consider. The implementation of interactive visual methods is an interesting notion that is discussed as potential future works in Section \ref{sec:conclusions}, but left outside the scope of this paper.

Machine learning techniques have gained recent interest in static analysis. One work proposed Large language models (LLMs) as an alternative to GUI-based interfacing to traverse visual graph representations of code-bases \cite{liu2024codexgraphbridginglargelanguage}. This method uses a graph of the code-base to enable the LLM to provide context-aware inputs as to the nature of code-bases. This is in contrast to traditional LLM input techniques which require each file to be input independently. Although LLMs can be useful in general tasks, traditional methods such as those presented in this paper are still required in complement to LLM-based analysis techniques due to the pitfalls such as hallucinations and over-/under-fitting training datasets. Another work presented an auto-encoder as a technique for parsing graph representations of code-bases rather then traditional algorithms \cite{8711909}. The work presented in this paper is focused on the workflow rather than a single specific method of graph characteristic extraction. Although a neural network solution could have been used instead of a traditional algorithm, drawbacks apply the same as the LLMs with respect to over-/under-fitting to the training dataset which could potentially introduce a risk to the completion of the overall project. As such, implementing such techniques were left outside the scope and are mentioned as future work potentials in Section \ref{sec:conclusions}.

%\cus{WIP: A comprehensive analysis on clustering-based graph analysis methods was recently done \cite{watteau2024advancedgraphclusteringmethods}.}

%\cus{WIP: Scalable static analysis techniques \cite{10.1145/3037697.3037744}.}

