\section{Introduction and Background}
\label{sec:intro}

% Background on HPC nework library design
High Performance Computing (HPC) is the use of many compute nodes in parallel to achieve a single task which allows for massive speedups of application performance. These nodes are connected locally over Infiniband, an HPC-specific Ethernet-based peer-to-peer network that focuses on high bandwidth and low latency \cite{infiniband_half_round_trip}. From the application's perspective, HPC networks are interacted with through high-level protocols including implementations of the Message Passing Interface (MPI), remote memory accesses, rendezvous protocols, streaming, etc. \cite{mpiforum}. These standards are very useful as they allow programmers to achieve what they want at the application level in the least amount of time possible, but does not require them to directly interface with the network.

The libraries that implement these protocols interact with the network through networking middleware libraries \cite{10.1007/978-3-540-72586-2_111}. These middlewares are highly optimized to provide the best performance on every possible set of hardware and are written in the C language to help achieve this \cite{7194625}. The trade-off for this performance is an equally high degree of source-code complexity which, coupled with a lack of technical documentation, makes the libraries notoriously difficult for new HPC library developers to understand \cite{1311050}. The primary advanced optimization techniques that are used include operating system (OS) bypassing and hardware-specific functionality exposure (e.g. atomic operations and thread-safety guarantees) \cite{ucx_paper}. OS bypassing in particular allows extreme speedups to occur through the direct management of hardware resources without kernel intervention, but results in the need to provide OS-level guarantees such as security which makes some aspects of the middleware more similar to OS code than typical high-level library codes \cite{ucx_paper, ucx_github}.

At the implementation level, these libraries heavily lean on the use of pre-processor manipulations such as macros and conditional compilations \cite{ISO:2001:IICb}. Macros are declarations that the pre-processor replaces instances of with a user-defined piece of code which can take anything as an argument (e.g. constants, functions, operators, other macros, etc.). The use of nested macros that are defined across multiple areas of the code-base are particularly common in HPC middleware libraries as they can be used to dynamically define codes inline at compilation time. This type of "meta-compilation" optimizes runtime performance directly through in-lining code to decrease jump counts and indirectly through the manipulation of compiler features which can shave single instructions off of the compiled code \cite{optimized_c_survey}. Conditional compilations on the other hand allow for certain sections of code to only be compiled if certain conditions are met. This can reduce the amount of "dead code" at runtime which can again save instructions \cite{optimized_c_survey}. The trend for maintainability of C codes is to move away from pre-processor macros in order to increase readability and reduce the number of nested dependencies \cite{1311050}, but the refactoring of hyper-optimized HPC codes specifically has shown in the past to be fruitless as the developers of these libraries care more about the extra performance they can achieve than increasing readability and maintainability \cite{10.1145/1145319.1145328}.

% Tools for development
In order to make it easier for new developers to understand the architecture of HPC code-bases, we need a methodology by which we can extract each component's design and how the components interact with one another. Existing solutions for the parsing of C code-bases primarily focus on the use of compiler plug-ins as they can use the pre-processor to expand macros and conditional declarations and piggyback on a compiler's semantic analyzers to parse the code \cite{Dudka2012-gm, sca_thesis, 10.1007/978-3-030-17872-7_6}. The primary drawback of parsing pre-processor expanded code when dealing with code-bases that heavily depend on recursive macros, as is the case in HPC middleware libraries, is the expanded code looks vastly different from their source-code counterparts. This is good for analyzing how the code will actually look as it runs in a specific hardware environment, but doesnâ€™t lend itself as well to understanding how the source-code is structured. It is possible for the context of the pre-processor directives to be saved throughout this process with compiler plug-ins, but it requires putting extra work in to remove the main advantage of the method.

An alternative to compiler-based parsing methods is to use generalized parsing tool generators, ANother Tool for Language Recognition (ANTLR) \cite{antlr4github, parr2013antlr} in the case of this work, to make generalized parsing workflows. The input to ANTLR is a grammar file that defines the semantic rules of any target language including linguistics, music, Morse code, programming languages, etc. The generalization of the workflows provided by ANTLR comes from the ability to swap the grammar files for different languages with relative ease. The grammar provided to these generators are context-sensitive grammars (Type-1) which means that they can determine how contextual information preceding a declaration effects the way it is interpreted. In contrast, utilities such as regular expressions (RegEx) that are common in simple parsing exercises are regular grammars (Type-3) \cite{hopcroft2006automata}. These regular grammars can only look at immediate locality within the string under study which is why more complex generators are necessitated for the parsing of dependencies in a complex languages. The primary drawback of this method is the degree of complexity involved with developing a grammar file that is compliant with and completely covers the specifications of the language being analyzed.

Other common approaches for the static analysis of C-based codes are to exclusively look at the include statements to draw a dependency graph \cite{cincludegraph} or to analyze the dependencies between modules on a per-file basis. An include-based dependency graph allows for unweighted graphical based methods of analysis such as automatic clustering to identify underlying architectures in the file dependencies \cite{792498_bunch, 693283_auto_clustering}. This method allows a scalable solution to get a broad understanding of the entire code-base, but falls short of providing enough insight as to the number or types of dependencies between different components in the system. Analyzing file dependencies on a per-file basis is on the opposite end of the spectrum as it provides in-depth details about the dependencies between two files, but doesn't provide a scalable view of the entire code-base. There has been work done with weighted dependency graphs to determine the best grouping of code modules into similarity clusters \cite{5286612}, but this was not expanded to the entire code-base as it was only looking at module-level relations, not general architecture patterns across large numbers of files.

% My methodology and contributions
The methodology presented in this work builds on the include-statement-based dependency graph method in order to provide a wholistic overview of middleware libraries source code as it is viewed by the developers. From this dependency graph, weightings will be added with respect to definitions of key components (macros, functions, structs, and unions) to capture the degree of inter-file reliance via the use of multiple components. From the dependency graph, clustering algorithms will be used in conjunction with hierarchical analysis techniques to determine architectural patterns within the code-base. The results of these hierarchical clustering experiments will then be analyzed for each middleware library in order to see what information about their architectures can be gleamed.

The primary contributions of this work include:
\begin{itemize}
    %\item A generalized method for the extraction of dependency graphs for C code.
    \item A comparison of the performance for two graph clustering algorithms for source code analysis using weighted and unweighted graph heuristics.
    \item An analysis of what general architectural patterns can be extracted from hierarchically clustered dependency graphs of HPC middleware libraries.
    \item An in-depth analysis on one of the libraries to demonstrate how dependency graphs can be used alongside other methods to extract more information about the underlying architecture.
    \item A macro-compliant C grammar file for ANTLR and generalized workflow for extracting the dependency graphs from C code with reproducibility artifacts provided at \url{https://github.com/curtis-shorts/csource-graph-plotter}
\end{itemize}

