%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clustering Algorithms}
\label{subsec:alg_description}

For the clustering algorithms, the methodology used was based on one proposed in prior works on file dependency clustering of code-bases \cite{792498_bunch, 693283_auto_clustering}. This method takes the nodes in the graph as a set that can be partitioned (segmented into non-empty subsets, or clusters, of the graph that collectively span the superset). The goal is to find a partition which best modularizes the code-base as indicated by the maximization of intra-dependencies within a cluster and the minimization of inter-dependencies between clusters. The heuristic used to determine this is dubbed the modularization quality (MQ) of the partition, the math of which is covered in Section \ref{subsec:mq_description}. From the MQ value, algorithms can be used to find the partition that best represents the architecture of the code-base.

Two algorithms were selected to find the partition with the best MQ value for k clusters in the partition. Running a monte-carlo of all possible partitions in the search space was also considered as an option, but it was experimentally determined that this was not feasible from a runtime perspective for a k value over 2. The first algorithm is gradient descent which starts with a randomly generated partition of k clusters where k is user-defined, then checks every possible neighboring partition to find the partition with the best MQ value. A neighboring partition is defined as a partition where all nodes except for one are in the exact same position. As with any gradient descent problem, multiple start points are required to find the best possible partition so each hierarchical set was run for 20 different randomly selected starting partitions. The second algorithm was a genetic algorithm 

Selection of the ideal k value can be done with an elbow-analysis of the curve of a plot with k as the x-axis and MQ as the y-axis. The elbow in the curve indicates the ideal k value for a given graph topology. The issue with this technique is that it is topology-dependent and therefore would require a sweep to be done for every sub-partition independently. As will be seen in Section \ref{sec:results}, the code-bases have over 300 combined sub-partitions that would need this analysis done. This leads to a scaling issue that makes this type of analysis untenable for the purposes of this work in contrast to works in which less hierarchical, and therefore less unique topologies, are considered. The route taken instead was to do static k value selection based on the number of nodes in a partition. The k values were determined experimentally as $k = 1$ for up to 4 nodes in a partition, 2 for up to 6, 3 for up to 15, 4 for up to 24, and 5 for 25 or more.

