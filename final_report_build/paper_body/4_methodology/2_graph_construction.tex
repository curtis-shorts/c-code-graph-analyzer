\subsection{Graph Construction}
\label{subsec:graph_construction}

The dependency data extracted from each file consisted of the include statements which were used to form the spine of the dependency graph and the declarations and instantiations of dependencies to create the weightings along each edge of the graph. The dependencies of interest were macros, functions, types, structs, global variables, and unions. The include statements and macro definitions were easily extracted from each file with RegEx parsing techniques as they use well-defined pre-processor directives, \#include and \#define respectively, which a regular grammar is more than capable of handling. The macro instantiations and the declarations/instantiations of the other dependency types required a context-sensitive grammar to extract them. ANTLR was chosen for this purpose with the details of the developed workflow being covered in Section \ref{subsec:antlr_workflow}.

For the graph's construction, files were initially chosen as the nodes as they are the fundamental unit that developers work with when partitioning the functional dependencies of their code. Files of interest in the code included *.c files which contain the implementations of code, *.h files which provide the code's interfaces so *.c files can access one another's functions and declarations, and *.inl files which contain inline functions. Inline functions are similar to macros in that they get expanded inline be the compiler to provide speedup benefits as the functions don't need to be called at runtime. Each file node was named by the path of the file relative to the source code directory (e.g. path/to/file.c) in order to ensure unique node names through the graph. After parsing was complete, each of these file nodes contained the file's name, the number of lines in the file, lists of definitions and instantiations of dependencies in the file, and a hash of the files that the node depends on according to the include statements. The lists of dependencies were split into macros and non-macros in order to analyze if there were differences between how each group was utilized across the code-base. The hash of file dependencies used the names of the files that were included as the key and the number of dependencies as the value. The number of dependencies file A had on file B was calculated by taking the list of instantiations in file A, removing the definitions in file A, then counting the number of instances remaining in file A that were defined in file B. This hash was then used to form the edge weightings for the graph.

Initial testing revealed that handling each file independently resulted in scaling issues with respect to the standpoints of algorithm runtime and outputting human-interpretable representations of the results as the code-bases consisted of hundreds of files. This required hierarchical techniques to be employed in order to reduce the total number of nodes in the graph. In order to achieve this, a secondary node type which represented clusters of related nodes was employed. These clusters can be thought of as black-boxes that appear as a single file to other nodes that contains the dependency sums of its component files. In implementation, these nodes simply contain a cluster name and a list of the nodes within the cluster. The cluster names were the last common path shared by the files in the cluster followed by an index to differentiate between clusters under the same directory (e.g. cluster/path:0, cluster/path:1). Since clusters appear as files to other nodes, clusters can also be nested with this method. The requirement for this construction is that any given file can only appear once in the graph meaning that a file cannot be a direct child of two clusters and a file in a cluster cannot appear outside the cluster.

Cluster nodes were used in two instances throughout the experiments: to group together *.c, *.h, and *.inl files of the same name (e.g. path/to/file.c, path/to/file.h, and path/to/file.inl) and to group together algorithm-identified clusters in a given sub-directory of the source code. Figure \ref{fig:graph_example} demonstrates what the network diagrams of the graphs look like for cluster nodes at different levels of the hierarchy. The grouping of files that had the same name except, for their type extension, was done to reduce the total file count of the code-base. These files are tightly coupled in their content by convention which is why this decision was deemed low-risk to the final results. Section \ref{subsec:workloads} provides insight into the effects this decision made on the size of the workloads. Figure \ref{fig:graph_example} a) and b) demonstrate the degree to which reducing the file counts can benefit the interpretability of the results with a) containing 13 nodes and b) containing 29, or a node count reduction of 55\%. The grouping of algorithm-identified clusters allowed for the clustering to happen hierarchically by folder. This means that in the example of file path/to/files/files.ext, files.ext would be hierarchically included in the hierarchical clusters of path/to/files:n, path/to:n, and path:n. This allowed for looking at the dependencies between files at different levels of the code-base's directory structure.

\input{figures/graph_design_examples/compilation}
